"Certificate of Cloudera Manager Agent will expire within 0 days" Alert on Cloudera Manager UI, Auto-TLS is not enabled in the Cluster
Labels: Administration ,CDP Private Cloud ,CDP Private Cloud Base ,Cloudera Manager ,Configure 
Apache JIRA(s): None
Attachment: None
Last Updated: 19/8/2023, 1:15:49 am
First Published: 8/8/2023, 10:40:41 am
Summary
Misconfiguration in Cloudera manager Agent Host: /etc/cloudera-scm-agent/config.ini, causing the Cloudera Manager performing agent host certification validation process. If the certificate had expired or within alerting threshold days of expiration, it will trigger CM Alert: Certificate of Cloudera Manager Agent will expire within 0 days. Even the the Cluster has not enabled Auto-TLS and Agent TLS setting: use_tls = 0.
Symptoms
CM Admin will see an alert on the affected Agent host: Certificate of Cloudera Manager Agent will expire within 0 days. The cluster has not enabled Auto-TLS for CM and CM Agent.

Under CM - Administration - Security - TLS settings, Auto-TLS is disabled

In affected host /etc/cloudera-scm-agent/config.ini, use_tls = 0 is configured.


photo.jpg

Applies to
1. misconfiguration in agent host:  /etc/cloudera-scm-agent/config.ini

2. The certificates, keystores, and password files generated by auto-TLS are presented in /var/lib/cloudera-scm-agent/agent-cert folder under Cloudera Manager Agent host, and certificate has expired or within threshold days of expiration.

Cause
The CM host: /etc/cloudera-scm-agent/config.ini has running configurations for for below TLS related parameters, even the "use_tls = 0" has been configured. This would be loaded when the CM agent service started, and will force the Agent certification validation process. If the agent certificate is close to expiration or already expired, it would trigger the alert according to the threshold.

verify_cert_file=
verify_cert_dir=
client_key_file=
client_keypw_cmd=
client_keypw_file=
client_cert_file=

In below cloudera_scm_agent.log example, we could see when the Agent service started, it has 

Instructions
1. Remove/commented out below related Agent TLS certification validation configs, therefor the certificate verification will not be performed.

# verify_cert_file=
# verify_cert_dir=
# client_key_file=
# client_keypw_cmd=
# client_keypw_file=
# client_cert_file= 

2. Remove/relocate the certificates, keystores, and password files generated by auto-TLS are presented in /var/lib/cloudera-scm-agent/agent-cert folder under Cloudera Manager Agent host. (Optional)

3. Restart Cloudera Manager Agent service;
# sudo systemctl restart cloudera-scm-agent

4. Restart Cloudera Manager Server service;
# sudo systemctl restart cloudera-scm-server

5. Verify if the Agent Certificate expirated alerts has been cleared from Host - All Host page


\n


No hdfs-site.xml file in local etc/hadoop/conf of gateway node
Labels: CDP Private Cloud ,Cloudera Manager ,Configure
Apache JIRA(s): None
Attachment: 11.png
Last Updated: 16/8/2023, 11:26:01 pm
First Published: 4/8/2023, 12:00:08 pm
Summary
Introduce the solution to fix the issue of no hdfs-site.xml file found in local etc/hadoop/conf of gateway node.
Symptoms
After deploy client configuration from CM -> HDFS, customer cannot find hdfs-site.xml file in local etc/hadoop/conf of gateway node with the below symptoms:

1. The deploy client configuration operation is successful without any error.

2. Currently all gateway nodes are missing hdfs-site.xml file.

Cause
According to the output from:

#ll /etc/hadoop/conf

#ll /etc/alternatives/hadoop-conf

 

We found the alternatives are pointing to conf.cloudera.CORE_SETTINGS currently hence the behaviour.

photo.jpg

Instructions
Beginning with Cloudera Manager 7.7.1 the Core Settings service is implemented in a new way that changes how it is added to the cluster and what occurs during an upgrade of Cloudera Manager.
By accident, the alternatives which should point to HDFS would change to Core Settings service and cause some issues for customer.
Most customer don’t know much about Core Settings service and “alternatives priority” settings, thus they would be confused by this issue.

To have alternatives pointing to HDFS, you need to increase the "alternatives priority" of HDFS service to a higher value than CORE_SETTINGS.
Post this deploy client configuration should be pointing to HDFS.

To verify execute below command before and post the change: 

#update-alternatives --display hadoop-conf

photo.jpg



\n



Additional Full DN Active Directory Attributes not Working in "Active Directory Account Properties"
Labels: CDP Private Cloud Base ,Cloudera Manager ,Configure ,Security
Apache JIRA(s): None
Attachment: None
Last Updated: 2/8/2023, 8:18:30 pm
First Published: 21/7/2023, 1:38:25 am
Summary
When adding attributes to the "Active Directory Account Properties" field in Cloudera Manager Security settings, regeneration of credentials can fail if the attribute contains ","'s. Some general account property attributes, such as "manager", contain full DN's containing ",". "Active Directory Account Properties" does not support entries with ",".
Symptoms
When adding attributes with commas you can receive the following exception upon regenerating Kerberos credentials:

 

"Active Directory Account Properties": accountExpires=0,objectClass=top,objectClass=person,objectClass=organizationalPerson,objectClass=user,manager=CN=gnu,OU=gnu,OU=gnu,OU=gnu,OU=gnu,DC=gnu,DC=gnu

Exception:

...

++ sed /str/d
++ echo rangertagsync/c1930-node4.coelab.cloudera.com@SUPPORT.COM
++ sed -e 's/\@SUPPORT.COM//g'
++ echo -n '"REDACTED"'
++ iconv -f UTF8 -t UTF16LE
++ base64 -w 0
++ echo 'manager: CN
OU: gnu
OU: gnu
OU: gnu
OU: gnu
DC: gnu
DC: gnu
'
SASL/GSSAPI authentication started
SASL username: test1@SUPPORT.COM
SASL SSF: 64
SASL data security layer installed.
ldap_add: Constraint violation (19)
additional info: 00002081: AtrErr: DSID-031517FA, #5:
0: 00002081: DSID-031517FA, problem 1005 (CONSTRAINT_ATT_TYPE), data 0, Att 15000a (manager)
1: 00002083: DSID-03151830, problem 1006 (ATT_OR_VALUE_EXISTS), data 0, Att b (ou):len 6
2: 00002083: DSID-03151830, problem 1006 (ATT_OR_VALUE_EXISTS), data 0, Att b (ou):len 6
3: 00002083: DSID-03151830, problem 1006 (ATT_OR_VALUE_EXISTS), data 0, Att b (ou):len 6
4: 00002081: DSID-031517FA, problem 1005 (CONSTRAINT_ATT_TYPE), data 0, Att 150019 (dc)


>>

Cause
Our script used to generate credentials, gen_credentials_ad.sh, uses "," as a delimiter between AD properties. This does not allow for additional properties containing full DN's or any value containing ","'s. 

Instructions
 
As attributes including ","'s are not supported in CM, our recommendation is to add comma separated attributes manually in AD. 



\n 



Hue AutoTLS Configuration Override
Labels: CDP Private Cloud Base ,Configure ,Hue
Apache JIRA(s): None
Attachment: None
Last Updated: 1/8/2023, 12:16:18 pm
First Published: 31/7/2023, 8:52:08 pm
Summary
When AutoTLS is enabled, it will enforce that the AutoTLS configurations are used for Hue, but this can be overridden.
Symptoms
There may be situations where the AutoTLS configuration does not work for Hue, especially because it, being Python based, used PEM files instead of JKS (or BCFKS) files to store certificates.

Here's an example of a Hue error that may require manually configuring TLS in spite of AutoTLS being enabled:

OpenSSL.crypto.Error: [('digital envelope routines', 'EVP_DecryptFinal_ex', 'bad decrypt'), ('PEM routines', 'PEM_do_header', 'bad decrypt')]
Instructions
Override the Auto-TLS configs in Cloudera Manager> Hue > Configuration > "Hue Service Advanced Configuration Snippet (Safety Valve) for hue_safety_valve.ini":

[desktop]
ssl_certificate=
ssl_private_key=
ssl_certificate_chain=
ssl_password=
ssl_password_script=<path to bash script that echoes password>



\n



Truncate table failure in Spark3
Labels: Administration ,Configure ,Operation ,Spark ,Troubleshooting
Apache JIRA(s): None
Attachment: None
Last Updated: 30/7/2023, 9:49:36 pm
First Published: 20/3/2023, 5:58:20 am
Summary
Truncate hive table is failing via Spark3 but works with Spark2
Symptoms
spark.sql("truncate table db.table") fails, even though the table purge option is already set to true==>

org.apache.spark.sql.AnalysisException: Operation not allowed: TRNCATE TABLE on external tables : 'db' . 'table' at org.apache.spark.sql.errors.QueryCompilationErrors$.truncateTableOnExternalTablesError(QueryCompilationErrors.scala:1921


Cause
truncate table for an external table is not supported from spark
https://spark.apache.org/docs/3.4.0/sql-ref-syntax-ddl-truncate-table.html#truncate-table

We had internal Jira CDPD-21614 - to support table truncate when the purge is set to true in Spark2 but it is not supported in Spark3 before.

Instructions
Workaround 1 :

  truncate the table from Hive

Workaround 2 :

  Change the table property from external table = true to false and then truncate the table from Spark. Once the table truncate is complete, change it back to true.

  ALTER TABLE db.table set TBLPROPERTISE ('EXETERNAL'='FALSE')

Workaround 3 :

  install the hotfix which includes the fix for CDPD-21614

  HOTFIX-5520 ==> 

  release_url: https://archive.cloudera.com/p/spark3/3.2.7172000.3
  
  
  
  
 \n 
 
 
 
 How to connect Microsoft SQL Server to Hive after setting the DSN with MIT KRB5 for Windows?
Labels: CDP Private Cloud ,Configure ,Hive ,Q & A
Apache JIRA(s): None
Attachment: None
Last Updated: 30/7/2023, 9:45:00 pm
First Published: 30/7/2023, 9:44:02 pm
Summary
How to connect Microsoft SQL Server to Hive after setting the DSN with MIT KRB5 for Windows?
Instructions
We could xconnect MS SQL Server to Hive using ODBC Driver v2.6.13 with the following Windows parameters:

ODBC Driver Configuration > Use SSPI

With environment variable:

KRB5CCNAME=%USERPROFILE%\krb5cache

After rebooting the server, regenerate the user KRB5 ticket from MIT 5 Kerberos GUI

Then connection will be successful



\n 




How to configure NiFi flow configuration for Disaster Recovery
Labels: Administration ,CDP Private Cloud Base ,Configure ,DataFlow ,Development 
Apache JIRA(s): None
Attachment: None
Last Updated: 27/7/2023, 1:09:24 pm
First Published: 23/7/2023, 12:50:11 pm
Summary
Can you setup a NiFi disaster recovery by setting up to NiFi cluster (NIFI-A and NIFI-B) with similer configuration? And in case of an issue can you move/ 

copy all the NiFi repositories, state directory, flow and configs from Cluster A to B?
Instructions
If you have two NiFi Clusters (NiFi-A and NiFi-B), and you take a backup of all the NiFi repositories, state directory, and configs then in case of NiFi-A failure. 
When you move these backup data to NiFi-A, it will not work, since NiFi manages the state and assigns a unique ID for each processor. Running tasks that cannot be 
executed from a different NiFi cluster may cause data corruption.
However, you can take backup of flow.xml.gz file and NiFi configuration file (nifi.properties and bootstrap.conf file) from NiFi-A cluster and using these files 
you can deploy the same flow design in NiFi-B cluster and run the jobs again (but from beginning).
Also, you can manage the Flow using NiFi Registry and store the latest version of flow design. NiFi Registry provides a central location for the storage and management 
of shared data flow resources across more than one instance of NiFi.



\n 



ERROR: "EncryptionException: Decryption Failed with Algorithm [AES/GCM/NoPadding]" during NiFi service startup
Labels: CDP Private Cloud Base ,CDP Public Cloud ,Cloudera Data Platform (CDP) ,Cloudera Enterprise Data Hub ,Configure 
Apache JIRA(s): None
Attachment: None
Last Updated: 27/7/2023, 1:16:38 pm
First Published: 23/7/2023, 1:02:43 pm
Summary
After upgrade, NiFi service does not start.
Symptoms
After upgrade, NiFi service does not start. The following error is displayed:

WARN org.apache.nifi.web.server.JettyServer: Failed to start web server... shutting down.
org.apache.nifi.encrypt.EncryptionException: Decryption Failed with Algorithm [AES/GCM/NoPadding]
at org.apache.nifi.encrypt.CipherPropertyEncryptor.decrypt(CipherPropertyEncryptor.java:78)
Cause
Caused by: javax.crypto.AEADBadTagException: mac check in GCM failed
There is new encryption algorithm introduced in new version of CFM/NIFI (nifi.sensitive.props.algorithm = NIFI_PBKDF2_AES_GCM_256). This issue occurs if CFM upgrade fails due to some reason (CSD jars not updated before upgrade, custom NARs are present, Network issue occurs, etc.). This interrupts the re-encryption of flow.xml.gz file with new algorithm.

Instructions
Follow below steps to encrypt the flow.xml.gz file manually:

Go to Cloudera Manager > NiFi > Configuration, and verify the property nifi.sensitive.props.algorithm. Confirm it is set to the latest algorithm NIFI_PBKDF2_AES_GCM_256. If not, set the value to NIFI_PBKDF2_AES_GCM_256
Take a backup of flow.xml.gz and flow.json.gz
Run the following command to encrypt flow.xml.gz (Update the below command with current CFM version and update the path of $WORK_DIR, default is /var/lib/nifi):
/opt/cloudera/parcels/CFM-<Version>/TOOLKIT/bin/encrypt-config.sh -n $WORK_DIR/config_backup/nifi.properties -f $WORK_DIR/flow.xml.gz -x -s YOUR-SENSATIVE-VALUE -b $WORK_DIR/config_backup/bootstrap.conf -A NIFI_PBKDF2_AES_GCM_256
* To find the value of YOUR-SENSATIVE-VALUE, by look in the latest directory for NIFI, /var/run/cloudera-scm-agent/process and search within the file proc.json for the value of key RANDOM_NIFI_SENSITIVE_PROPS_KEY.
Verify the flow.xml.gz after running the command, whether or not it has NiFi user permissions
Start NiFi services.



\n 



How to manually remove Navigator Encryption at disk level
Labels: Configure ,Navigator Encrypt
Apache JIRA(s): None
Attachment: None
Last Updated: 27/7/2023, 1:09:09 pm
First Published: 20/7/2023, 10:43:26 pm
Summary
This article describes how to manually remove Navigator Encryption at the disk level.
Instructions
Check the below command outputs to get full information on the encrypted disk.

- sudo navencrypt-collect > navencrypt.info
- output of "ls -altr /dev/disk"
- output of “/etc/init.d/navencrypt-mount”
- output of "navencrypt --version"
- cat /etc/navencrypt/ztab/, /etc/navencrypt/control, and /etc/navencrypt/keytrustee/deposits/<path_to_dev_uuid>
 Steps to manually remove Navigator Encryption on the disk level:

systemctl stop navencrypt-mount
Remove the /path/to/sdxx entries from the ztab and control file and deposits
systemctl start navencrypt-mount
Check status: sudo /etc/init.d/navencrypt-mount status
Note : Above steps may lead to data loss on the encrypted disk.



\n 



How to add custom hive hook
Labels: CDP Private Cloud Base ,Configure ,Hive
Apache JIRA(s): None
Attachment: None
Last Updated: 25/7/2023, 3:02:28 pm
First Published: 19/6/2023, 7:21:17 am
Summary
This article describes about how to append a custom hive hook using hive.exec.post.hooks parameter.
Instructions
To append a custom hive hook value to the existing value of hive.exec.post.hooks without affecting the default settings, please follow the instructions below:

CM > hive_on_tez > Configuration > Hive Service Advanced Configuration Snippet (Safety Valve) for hive-site.xml >
Add the setting as:
Name:hive.exec.post.hooks
Value:org.apache.hadoop.hive.ql.hooks.HiveProtoLoggingHook,org.apache.atlas.hive.hook.HiveHook,new-custom-hive-hook
(Basically, you can add it as comma separated values)



\n 



How Oozie can retry database operations on failing database connectivity or errors?
Labels: Configure ,Oozie ,Operation ,Tutorial
Apache JIRA(s): None
Attachment: None
Last Updated: 25/7/2023, 7:51:51 pm
First Published: 25/7/2023, 7:47:58 pm
Summary
A typical use case is to keep the Oozie running while the backend MySQL goes for a restart during maintenance.
Instructions
Use the below properties to tolerate MySQL restart (unplanned outages)

oozie.service.JPAService.retry.initial-wait-time.ms
oozie.service.JPAService.maximum-wait-time.ms
oozie.service.JPAService.retry.max-retries

Example:-
oozie.service.JPAService.retry.initial-wait-time.ms=30000 ( 30 seconds initial wait time)
oozie.service.JPAService.maximum-wait-time.ms=600000 ( 10 minutes )
oozie.service.JPAService.retry.max-retries=20

Initially, oozie should wait for 30 seconds before retrying to write to DB.
After the retry fails; it should wait for 60 seconds [gets double of previous wait time]
This wait before a retry should not exceed 10 minutes for any consecutive attempts [Maximum wait time between database retry attempts]



\n 



How to make variable in a Zeppelin note unshared by other note
Labels: CDP Private Cloud ,Configure ,Zeppelin
Apache JIRA(s): None
Attachment: None
Last Updated: 24/7/2023, 11:44:16 am
First Published: 20/12/2022, 2:39:29 pm
Summary
How to make variable in a Zeppelin note unshared by other note
Symptoms

The variable defined in one notebook can be shared by another notebook. How to restrict the variable per notebook scope.

Note1 of livy interperter
INPUT:

%livy.pyspark 
num1 = 1.5
num2 = 6.3
sum = num1 + num2
print('The sum of {0} and {1} is {2}'.format(num1, num2, sum))
OUTPUT:

The sum of 1.5 and 6.3 is 7.8
 

Note 2 of  livy interperter
INPUT:

%livy.pyspark
print('The sum of {0} and {1} is {2}'.format(num1, num2, sum))
OUTPUT:

The sum of 1.5 and 6.3 is 7.8
 
Cause
If the interpreter is running in scoped mode then all users will share the same JVM process.

https://zeppelin.apache.org/docs/0.8.0/usage/interpreter/interpreter_binding_mode.html#isolated-mode

Instructions
Change the interpreter binding mode to => Isolated mode

https://docs.cloudera.com/cdp-private-cloud-base/7.1.7/using-zeppelin/topics/zeppelin_modify_interpreter_settings.html



\n 



"ERROR [main]: exec.DDLTask (DDLTask.java:failed(542)) - java.lang.OutOfMemoryError: Java heap space" or "Exception thrown while processing using a batch size xxxxx" when running Hive MSCK Repair table
Labels: CDP Private Cloud ,CDP Private Cloud Base ,Configure ,Hive ,Perform
Apache JIRA(s): None
Attachment: None
Last Updated: 18/7/2023, 10:48:48 pm
First Published: 14/9/2019, 11:39:37 am
Summary
MSCK repair on a Hive table fails with error "ERROR [main]: exec.DDLTask (DDLTask.java:failed(542)) - java.lang.OutOfMemoryError: Java heap space" or "Exception thrown while processing using a batch size xxxxx"
Symptoms
MSCK repair on a Hive table fails with the following error:

hive> msck repair table <Table_Name>; 

FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Java heap space 
2018-10-17 02:25:59,927 ERROR [main]: exec.DDLTask (DDLTask.java:failed(542)) - java.lang.OutOfMemoryError: Java heap space 
at java.util.Arrays.copyOf(Arrays.java:3236) 
at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118) 
at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93) 
at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153) 
at org.apache.thrift.transport.TSaslTransport.write(TSaslTransport.java:476) 
at org.apache.thrift.transport.TSaslClientTransport.write(TSaslClientTransport.java:37) 
at org.apache.hadoop.hive.thrift.TFilterTransport.write(TFilterTransport.java:72) 
at org.apache.thrift.protocol.TBinaryProtocol.writeI16(TBinaryProtocol.java:169) 
at org.apache.thrift.protocol.TBinaryProtocol.writeFieldBegin(TBinaryProtocol.java:124) 
at org.apache.hadoop.hive.metastore.api.FieldSchema$FieldSchemaStandardScheme.write(FieldSchema.java:530) 
at org.apache.hadoop.hive.metastore.api.FieldSchema$FieldSchemaStandardScheme.write(FieldSchema.java:480) 
at org.apache.hadoop.hive.metastore.api.FieldSchema.write(FieldSchema.java:418) 
at org.apache.hadoop.hive.metastore.api.StorageDescriptor$StorageDescriptorStandardScheme.write(StorageDescriptor.java:1451) 
at org.apache.hadoop.hive.metastore.api.StorageDescriptor$StorageDescriptorStandardScheme.write(StorageDescriptor.java:1278) 
at org.apache.hadoop.hive.metastore.api.StorageDescriptor.write(StorageDescriptor.java:1144) 
at org.apache.hadoop.hive.metastore.api.Partition$PartitionStandardScheme.write(Partition.java:1062) 
at org.apache.hadoop.hive.metastore.api.Partition$PartitionStandardScheme.write(Partition.java:919) 
at org.apache.hadoop.hive.metastore.api.Partition.write(Partition.java:815) 
at org.apache.hadoop.hive.metastore.api.AddPartitionsRequest$AddPartitionsRequestStandardScheme.write(AddPartitionsRequest.java:768) 
at org.apache.hadoop.hive.metastore.api.AddPartitionsRequest$AddPartitionsRequestStandardScheme.write(AddPartitionsRequest.java:676) 
at org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.write(AddPartitionsRequest.java:586) 
at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$add_partitions_req_args$add_partitions_req_argsStandardScheme.write
(ThriftHiveMetastore.java:56727) 
at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$add_partitions_req_args$add_partitions_req_argsStandardScheme.write
(ThriftHiveMetastore.java:56691) 
at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$add_partitions_req_args.write(ThriftHiveMetastore.java:56642) 
at org.apache.thrift.TServiceClient.sendBase(TServiceClient.java:71) 
at org.apache.thrift.TServiceClient.sendBase(TServiceClient.java:62) 
at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.send_add_partitions_req(ThriftHiveMetastore.java:1614) 
at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.add_partitions_req(ThriftHiveMetastore.java:1606) 
at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.add_partitions(HiveMetaStoreClient.java:629) 
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

<Problem> Msck repair table fails.
<Error> WARN org.apache.hadoop.hive.metastore.utils.RetryUtilities$ExponentiallyDecayingBatchWork: [HiveServer2-Background-Pool: Thread-24058]: Exception thrown while processing using a batch size 39951
Exception thrown while processing using a batch size 19975
Exception thrown while processing using a batch size 9987
Exception thrown while processing using a batch size 4993
 
Applies to
CDP
Hive
Configuration
Performance
Linux
Cause
This issue occurs when the heap size set for Hive client is low or need a performance tuning.

Instructions
To resolve this issue, do either of the following:

Increase the Hive Client heap to a higher value.
OR

Whitelist and update the values at session level.
set hive.msck.path.validation=ignore; 
Set hive.msck.repair.batch.size=300   



\n 



unable to access hive table from spark sql but able access the same table from hive
Labels: Administration ,CDP Private Cloud Base ,CDP Public Cloud ,Configure ,Hive 
Apache JIRA(s): None
Attachment: None
Last Updated: 18/7/2023, 10:20:35 pm
First Published: 8/5/2023, 5:52:35 pm
Summary
Users are trying to access the hive table from spark-shell but they are getting permission issues on the database level. Table-level access has been given to the users on Ranger, the same tables are accessible from the hive beeline but not from SPARK SQL
Symptoms
Users are trying to access the hive table from Spark SQL but getting permission issues on the database level.

We have given the table-level access on Ranger to the users, and while running the jobs from hive beeline we are able to run the file but when using Spark SQL we can see the below permission denied issue.
This issue is coming on the database level, not at the table level. If we look carefully we can see the user is not having the  While running the queries from Spaek the users are unable to view or select the desired database. Spark needs an extra policy for this, which is as per the design.


0: jdbc:hive2://hostname> select count(*) from testDb.test_table;
going to print operations logs
printed operations logs
going to print operations logs
INFO : Compiling command(queryId=hive_20230428171159_1d65bc40-a9ea-4141-91ae-19d25b183583): select count(*) from testDB.test_table
INFO: Semantic Analysis Completed (retrial = false)

+---------+
| _c0 |
+---------+
| 551004 |
+---------+
1 row selected (54.57 seconds)
0: jdbc:hive2://<hostname>>

----------------------------
spark ERROR
Caused by: org.apache.hadoop.hive.metastore.api.MetaException: Permission denied: user [test] does not have [SELECT] privilege on [testDB]
at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_table_req_result$get_table_req_resultStandardScheme.read(ThriftHiveMetastore.java)
at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_table_req_result$get_table_req_resultStandardScheme.read(ThriftHiveMetastore.java)

Instructions
We can resolve this issue by providing full database access to users, but this could lead to a big security challenge.
But we can also follow the below steps to resolve this issue without providing full database access. 
1. add the group or user in Ranger Policy "all - database" and give only select permission.
2. verify the access via Spark SQL by running the "show databases" command.  
3. rerun the job from spark-shell, this should run successfully.
By providing select privileges on all databases we are restricting the users to only view or select the database same as "use <database-name>. The users won't have any privileges like dropping the database or any other operations. We don't have to change any policies at the table level.
By the error below we can see that the users do not have any select privilege in the database, here "testDB" is the database, not the table.
Permission denied: user [test] does not have [SELECT] privilege on [testDB]



\n 



ERROR: HBase Canary failing with "The length of family name is bigger than 127"
Labels: Configure ,HBase ,Operation ,Troubleshooting
Apache JIRA(s): None
Attachment: None
Last Updated: 13/7/2023, 10:23:47 pm
First Published: 10/7/2023, 3:23:20 pm
Summary
HBase canary test is throwing following alert. org.apache.hadoop.hbase.HBaseIOException: java.lang.IllegalArgumentException: The length of family name is bigger than 127 HBase region health: 0 unhealthy regions
Cause
HBase key is a serialized byte array, so it is not possible have length of column family name larger than 127.

Instructions
- Avoid having larger column family name whose length exceeds 127.
- If necessary, migrate the table whose column family length does not exceed 127.
  
  
  
\n 



Unable to the Yarn Queue Manager and it fails with the UI error -"fail to register cluster" as well reports UI error as "Failed to get versions", "Failed to get queues" error messages.
Labels: CDP Private Cloud ,Configure ,YARN Queue Manager UI
Apache JIRA(s): None
Attachment: None
Last Updated: 11/7/2023, 7:43:44 pm
First Published: 3/1/2022, 7:15:49 pm
Summary
This article describes about the cause of the UI error received while accessing the YARN Queue Manager and how to resolve it.
Symptoms
Accessing the Yarn queue manager fails with UI error -"fail to register cluster" , "Failed to get versions", "Failed to get queues".
 
The Yarn Queue Manager logs the below error is seen at the same time while accessing the Yarn Queue Manager UI.
ERROR com.cloudera.cpx.server.api.utils.HttpClientInstance: https://10.10.10.10:8090/ws/v1/custer is not up or returns an error
com.sun.jersey.api.client.ClientHandlerException: javax.net.ssl.SSLHandshakeException: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target
 at com.sun.jersey.client.urlconnection.URLConnectionClientHandler.handle(URLConnectionClientHandler.java:155)
....

Caused by: javax.net.ssl.SSLHandshakeException: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpat.SunCertPathBuilderException: unable to find valid certification path to requested target
....
Caused by: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target
Applies to
Yarn Queue Manager.
CDP.
Yarn.
Cause
 The error is a client SSL handshake exception which is caused if the Yarn Queue Manager is not SSL enabled whereas the Yarn service is ssl enabled, Yarn Queue Manager is a client to Yarn service.
 Yarn Queue Manager client needs to have the trust store path and password configured correctly.

Instructions
Refer to the docs in steps 6 to step 12 describes about how to configure TLS/SSL for the Yarn Queue manager.



\n 



Knox gateway not redirecting to proper descriptors URL
Labels: Administration ,CDP Private Cloud ,Configure ,Knox
Apache JIRA(s): KNOX-2616
Attachment: None
Last Updated: 11/7/2023, 11:24:16 am
First Published: 14/12/2021, 10:43:41 pm
Summary
Knox does not add trailing slash in the URL causing issues for services UI, when accessing via Knox gateway.
Symptoms

In CDP version 7.1.7,  when trying to access services UI ( ranger, solr and zeppelin ) from knox gateway . The error i see in knox-gateway logs is " failed to match path"

Cause
Sync issue between Knox's data dir content and the parcel. 

https://issues.apache.org/jira/browse/KNOX-2616

Instructions
Either one of below can be followed to address the issue.

1. We can manually edit the service.xml file for each affected services to add additional slash in the service context field.

For instance service.xml for rangerui. 

#cd /var/lib/knox/gateway/data/services/rangerui/0.5.0

#vim services.xml  ## add a forward slash in the context property from "/ranger" to "/ranger/"

=============
<service role="RANGERUI" name="rangerui" version="0.5.0">
<metadata>
<type>UI</type>
<context>/ranger/</context>
=============

2. Below steps should help sync Knox's data dir content with the parcel.

rm -rf /var/lib/knox/gateway/data/applications/*
cp -R /opt/cloudera/parcels/CDH/lib/knox/data/applications/* /var/lib/knox/gateway/data/applications/

rm -rf /var/lib/knox/gateway/data/services/*
cp -R /opt/cloudera/parcels/CDH/lib/knox/data/services/* /var/lib/knox/gateway/data/services/

rm -rf /var/lib/knox/gateway/data/deployments/*



\n 



Re-Index Atlas Collections in Infra Solr
Labels: Atlas ,CDP Private Cloud ,Configure ,Solr
Apache JIRA(s): None
Attachment: None
Last Updated: 11/7/2023, 7:29:59 pm
First Published: 15/12/2021, 8:14:10 pm
Summary
This article explains how to re-index Atlas Collections ( vertex_index , fulltext_index , edge_index ) in Infra Solr.
Symptoms
While running repair_index.py to ReIndex Atlas Collections in Infra Solr, below Solr Error is encountered:

Caused by: org.apache.solr.client.solrj.impl.CloudSolrClient$RouteException: Error from server at https://servername/solr/vertex_index_shard2_replica_n2: Expected mime type application/octet-stream but got text/html.

 
HTTP ERROR 401 Unauthorized access

Cause
In CDP DC distribution, repair_index.py script is replaced with new feature added in Atlas - https://issues.apache.org/jira/browse/ATLAS-4015

Instructions
Navigate to CM UI -> Atlas -> Configuration -> Add below in "Atlas Server Advanced Configuration Snippet (Safety Valve) for conf/atlas-application.properties"

atlas.rebuild.index=true
atlas.patch.numWorkers=22
atlas.patch.batchSize=1000

This will require Atlas Service Restart. Atlas will be available after completing reindex post restart.

You can monitor application.log for thread 'reindex-' to see the progress. Sample logs: 

INFO - [main:] ~ ReIndexPatch: Starting... (ReIndexPatch:63)
INFO - [main:] ~ repairElements.execute(): vertex_index: Starting... (ReIndexPatch$ReindexPatchProcessor:103)
INFO - [reindex-19:] ~ Processed: 17 (ReIndexPatch$ReindexConsumer:191)
INFO - [reindex-13:] ~ Processed: 13 (ReIndexPatch$ReindexConsumer:191)
INFO - [reindex-13:] ~ Total: Commit: 13 (ReIndexPatch$ReindexConsumer:172)
....
....
INFO - [main:] ~ repairElements.execute(): vertex_index: Done! (ReIndexPatch$ReindexPatchProcessor:113)
INFO - [main:] ~ repairElements.execute(): edge_index: Done! (ReIndexPatch$ReindexPatchProcessor:113)
INFO - [main:] ~ ReIndexPatch: Done! (ReIndexPatch:71)
 
Note : Make sure that config "atlas.rebuild.index=false" is set after successful re-index completion, else atlas will try to re-index on every restart which delays startup.




\n 



How to mitigate HTTP Strict Transport Security (HSTS) security concern in HandleHttpRequest processor port
Labels: Configure ,DataFlow ,Development ,NiFi ,Security
Apache JIRA(s): None
Attachment: None
Last Updated: 12/7/2023, 9:25:41 pm
First Published: 10/7/2023, 10:19:53 am
Summary
When a HandleHttpRequest is configured to listen in a TLS port, web server security scanners might alert that Strict-Transport-Security Header is Not Set.
Symptoms
Security scanner reports Strict-Transport-Security Header Not Set an alert for the HandleHttpRequest processor port when SSL context service is configured.

Cause
Missing Strict-Transport-Security header in the HandleHttpResponse processor

Instructions
Set a new property for the HandleHttpResponse processor, for example: 

Name: Strict-Transport-Security
Value: max-age=31536000; includeSubDomains

Now the HTTP response will contain this header: 

< HTTP/1.1 200 OK
< Date: Fri, 07 Jul 2023 17:07:22 GMT
< Strict-Transport-Security: max-age=31536000; includeSubDomains
< Transfer-Encoding: chunked
< Server: Jetty(9.4.49.v20220914)
See Also
More details about this header can be found in: 

https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Strict-Transport-Security




\n 



How to make configuration changes effective to HDFS NN without Namenode role restart in CM Web UI ?
Labels: CDP Private Cloud Base ,Configure ,HDFS ,Q & A ,Tutorial
Apache JIRA(s): None
Attachment: None
Last Updated: 11/7/2023, 1:22:05 pm
First Published: 11/7/2023, 1:14:03 pm
Summary
This tutorial contains the steps to make configuration changes effective to HDFS NN without Namenode role restart in CM Web UI.
Instructions
For example, if changes related to below properties [1] needs to take effect, then we can either make the changes followed by restart of NN hosts via CM Web UI or follow an alternate approach as stated below to avoid downtime i.e., without restarting NN roles in CM.
[1] : 
dfs.namenode.replication.work.multiplier.per.iteration
dfs.namenode.replication.max-streams
dfs.namenode.replication.max-streams-hard-limit 
Update/Modify the hdfs-site.xml present in both NN process directory something like below for above mentioned properties.
<property>
     <name>dfs.namenode.replication.max-streams</name>
     <value>100</value>
</property>
<property>
     <name>dfs.namenode.replication.max-streams-hard-limit</name>
     <value>150</value>
</property> 
<property>
     <name>dfs.namenode.replication.work.multiplier.per.iteration</name>
     <value>50</value>
</property>
Then, issue below command to refresh the Namenode with above configs immediately.
"hdfs dfsadmin -reconfig namenode ANN/SBNN:8020 start"



\n 



ERROR: [Producer clientId=connector-producer-example-connector-0] Bootstrap broker node1.example.com:9093 (id: -2 rack: null) disconnected in Kafka Connect for Third Party Source Connector with secure Kafka
Labels: CDP Private Cloud ,Configure ,Kafka Connect ,Security
Apache JIRA(s): None
Attachment: None
Last Updated: 6/7/2023, 9:08:32 pm
First Published: 27/4/2021, 11:18:08 pm
Summary
As of the time of this article, Cloudera directly supports two Sink Connectors as part of Kafka Connect, but no Source Connectors. In a secure environment, certain security properties which are automatically configured by Cloudera Manager for Sink Connectors must be manually added for Source Connectors.
Symptoms
Source Connectors fail with errors like below.

[Producer clientId=connector-producer-example-connector-0] Bootstrap broker broker1.example.cloudera.com:9093 (id: -2 rack: null) disconnected
Applies to
Third Party Source Connectors deployed via Kafka Connect in CDP 7.1.7 and lower.

Cause
The producers and consumers of a connector in Kafka Connect are configured using the producer. and consumer. prefixed properties in the Kafka Connect configuration. Cloudera Manager automatically generates consumer. security properties to enable support for the Cloudera provided Sink connectors, but the producer. properties are not automatically generated.

In the case of a Kafka cluster with SSL or Kerberos enabled, this results in failures of the connectors to communicate with the Kafka brokers as they are not configured for the correct security protocol.

Instructions
 
Provide security settings with the producer. prefix in Cloudera Manager > Kafka > Configuration > Kafka Connect Advanced Configuration Snippet (Safety Valve) for connect-distributed.properties

For example:

producer.ssl.truststore.location = /etc/security/kafka_truststore.jks
producer.ssl.truststore.password = ${cm-agent:ENV:CONNECT_SSL_SERVER_TRUSTSTORE_PASSWORD}
producer.sasl.mechanism = GSSAPI
producer.sasl.kerberos.service.name = kafka
producer.security.protocol = SASL_SSL
producer.sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required \
 useKeyTab=true \
 storeKey=true \
 keyTab="${cm-agent:keytab}" \
 principal="${cm-agent:ENV:kafka_connect_service_principal}";

Note that not all of the properties above may be required for the specific Kafka security protocol used in the cluster. For more information about configuring a Kafka producer for different security protocols, please refer to the documentation below.

See Also
Kafka Authentication



\n 



How to use a custom JWT cookie when multiple cluster UI sessions are configured for Knox SSO - CDP Data Center
Labels: CDP Data Center ,Configure ,Knox ,Operation ,Security 
Apache JIRA(s): None
Attachment: None
Last Updated: 6/7/2023, 10:32:57 pm
First Published: 20/10/2020, 3:49:02 am
Summary
This article describes why Knox UI login does not work seemingly when logging into multiple HDP environments at the same time due to the default cookie name "hadoop-jwt", and how to. resolve this issue.
Symptoms
When accessing two clusters (with Knox SSO enabled and default cookie name as hadoop-jwt), only one cluster session runs and the session to the other cluster is signed off automatically.

While trying to log in to the Knox UI by clicking the SIGN IN button, it takes you right back to the Sign in page. On clearing the cookies/ cache, login is successful, but the issue reappears later. On trying through a new incognito session, the login is successful.

Knox, by default, uses hadoop-jwt as the cookie name for SSO authentication for all the Hadoop services. If there are multiple clusters configured for SSO authentication, the cookie name must be changed, as using hadoop-jwt cookie name conflicts with the cookie issued by Knox of another cluster.

Cause
This issue occurs because of the default cookie name as hadoop-jwt. When using this default cookie name, while accessing a cluster with a certain domain (such as cluster1.hortonworks.com), the web browser will use that token to identify with that cluster. However, when accessing the other cluster (such as newcluster.hortonworks.com), the first token will be invalid and the session will be signed off.

Instructions
To resolve this issue, perform the following steps to change the default cookie name in CDP:

Log in to the Cloudera Manager

Navigate to Knox > Knox Gateway Advanced Configuration Snippet (Safety Valve) for conf/cdp-resources.xml

Append below config (as text) to change cookie name for knoxsso:
<property>
<name>knoxsso</name>
<value>providerConfigRef=knoxsso#KNOXSSO:knoxsso.token.ttl=86400000#KNOXSSO:knoxsso.cookie.name=hadoop-jwt-prod#app:knoxauth</value>
</property>
<property>
<name>providerConfigs:homepage</name>
<value>role=federation#federation.name=SSOCookieProvider#federation.enabled=true#federation.param.sso.cookie.name=hadoop-jwt-prod</value>
</property>
<property>
<name>providerConfigs:metadata</name>
<value>role=federation#federation.name=SSOCookieProvider#federation.enabled=true#federation.param.sso.cookie.name=hadoop-jwt-prod</value>
</property>
<property>
<name>providerConfigs:sso</name>
<value>role=federation#federation.name=SSOCookieProvider#federation.enabled=true#federation.param.sso.authentication.provider.url=
https://c416-node4.coelab.cloudera.com:8443/gateway/knoxsso/api/v1/websso#federation.param.sso.cookie.name=hadoop-jwt-prod</value>
</property>
Note: 
 

Ensure changing the provider.url under providerConfigs:sso.
As cookies are identified by end services, like here with Knox services like homepage,metadata and cdp-proxy (which is using providerConfigs:sso), we should set the property sso.cookie.name on these topologies.
Restart Knox and verify the access with a new cookie name (set cookie name differently for each environment cluster).



\n 



ERROR: "Failed to execute tez graph..." when submitting large DAGs in Tez AM
Labels: CDP Private Cloud ,Configure ,Hive
Apache JIRA(s): None
Attachment: None
Last Updated: 3/7/2023, 2:48:16 pm
First Published: 29/10/2021, 12:30:23 pm
Summary
This article describes why Hive on Tez jobs running on CDP fails, and the way to resolve this issue.
Symptoms
Hive on Tez jobs running on CDP fails with the following error:

2021-10-26 14:10:22,951 ERROR org.apache.hadoop.hive.ql.exec.tez.TezTask: [HiveServer2-Background-Pool: Thread-44087]: Failed to execute tez graph.
java.lang.NullPointerException: java.lang.NullPointerException
 at org.apache.tez.dag.api.client.rpc.DAGClientAMProtocolBlockingPBServerImpl.submitDAG(DAGClientAMProtocolBlockingPBServerImpl.java:169)
You can also observe the following stack trace for the queries running from Beeline:

2021-10-26 14:10:22,961 ERROR org.apache.hadoop.hive.ql.Driver: [HiveServer2-Background-Pool: Thread-44087]: FAILED: Execution Error, return code 1 from org.apache.
hadoop.hive.ql.exec.tez.TezTask
2021-10-26 14:10:22,962 INFO org.apache.hadoop.hive.ql.Driver: [HiveServer2-Background-Pool: Thread-44087]: Completed executing command(queryId=hive_20211026141005_
0a705f2a-402d-4c06-86e2-d650e2fa42d1); Time taken: 11.52 seconds
2021-10-26 14:10:22,962 INFO org.apache.hadoop.hive.ql.lockmgr.DbTxnManager: [HiveServer2-Background-Pool: Thread-44087]: Stopped heartbeat for query: hive_2021102
6141005_0a705f2a-402d-4c06-86e2-d650e2fa42d1
2021-10-26 14:10:22,970 ERROR org.apache.hive.service.cli.operation.Operation: [HiveServer2-Background-Pool: Thread-44087]: Error running hive query: 
org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.tez.TezTask

at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:352) ~[hive-service-3.1.2000.7.0.3.0-79.jar:3.1.2000.7.0.3.0-79]
Cause
If you observe closely in the HiveServer2 logs just before Nullpointer exception, you can see the actual message as follows:

2021-10-26 14:10:16,006 INFO org.apache.tez.client.TezClient: [HiveServer2-Background-Pool: Thread-44087]: Send dag plan using YARN local resources since it's too 
large, dag plan size=80933509, max dag plan size through IPC=61865984, max IPC message size= 67108864
2021-10-26 14:10:16,443 INFO org.apache.hadoop.hive.ql.exec.Task: [HiveServer2-Background-Pool: Thread-44087]: Dag submit failed due to java.lang.NullPointer
Exception at org.apache.tez.dag.api.client.rpc.DAGClientAMProtocolBlockingPBServerImpl.submitDAG(DAGClientAMProtocolBlockingPBServerImpl.java:169)
It seems the query DAG size (ipc.maximum.data.length) exceeded here, and because of that, we are seeing this NPE error.

Instructions
To resolve this issue, increase the following parameter value such that it can allocate the DAG and run successfully.

Go to Cloudera Manager > HDFS > Configrations:hdfs-(service-wide) > Cluster-wide Advanced Configuration Snippet (Safety Valve) for core-site.xml
Increase ipc.maximum.data.length to 134217728
Restart the stale services
Note: Set ipc.maximum.data.length in core-site.xml as stated above rather than hdfs-site.xml in HDFS configurations.
Issue is fixed in CDP 7.1.7 and later versions.



\n 



Configuration for fs.trash.interval is not working in Public Cloud
Labels: Administration ,CDP Public Cloud ,Cloud ,Configure ,HDFS
Apache JIRA(s): None
Attachment: None
Last Updated: 4/7/2023, 11:43:21 am
First Published: 3/7/2023, 2:26:55 pm
Summary
- Configuration for fs.trash.interval is not taking effect while using ADLS Storage in Cloud Environments.
Symptoms
- HDFS Trash policy is supposed to be working on only the Primary HDFS storage Filesystem and not on the external Secondary Storages.
- Therefore, unless you add the option of '-skipTrash' while removing the files from the cluster, the files will not be removed from the ADLS directories.
- In order to remove these files, you may need to take the manual intervention through some automated cron jobs or scripts for existing files. 
- The files which are going to be removed in the future should be using '-skipTrash' option with your HDFS commands.

Instructions
Cloudera Manager > HDFS > Configuration > fs.trash.interval=1 

hadoop fs -rm -skipTrash adl://your_account.azuredatalakestore.net/user/user_name/.Trash/current/



\n 



How to sync Users/groups using Cloudera restAPI
Labels: Administration ,CDP Public Cloud ,Cloudera Management Console ,Configure ,User Management
Apache JIRA(s): None
Attachment: None
Last Updated: 30/6/2023, 2:58:35 pm
First Published: 24/3/2023, 5:58:59 pm
Summary
How to automate user management tasks. Adding machine users to the environment using restAPI via Postman. Once the user is added, synchronizing the users to using restAPI to avoid manual synchronization.
Instructions
Synchronizes environments with all users and groups state with CDP

Configure CDPCURL to work the below curl commands. (To configure the CDPCURL follow the document).
Create Machine user using restAPI:
cdpcurl --profile default -X POST -d '{"machineUserName":"<apitestuser>"}' https://iamapi.us-west-1.altus.cloudera.com/iam/createMachineUser
Assign resource and resource role to the Machine user using restAPI:(optional if required to assign role to the Machine User)
cdpcurl --profile default -X POST -d '{"machineUserName":"<apitestuser>", "resourceCrn" : 
"<crn:cdp:environments:us-west-1:558bc1d2-8867-4357-8524-311xxxxxxx:environment:b0eeb422-85ed-xxxxxxxxx-xxxxxx>", 
"resourceRoleCrn" : "crn:altus:<iam:us-west-1>:altus:resourceRole:<EnvironmentUser>" }' 
https://iamapi.us-west-1.altus.cloudera.com/iam/assignMachineUserResourceRole
Sync Users/Groups to the environment:
cdpcurl --profile default -X POST -d '{"environmentName": "<environmentname>"}' https://api.us-west-1.cdp.cloudera.com/api/v1/environments2/syncAllUsers
Make necessary changes to the entities, given above is just an example.

Here is a reference document to all the APIs



\n 



NiFi : Authorizers.xml : Composite Implementations : Which one to choose between CompositeUserGroupProvider vs CompositeConfigurableUserGroupProvider
Labels: Administration ,Configure ,NiFi ,NiFi Registry ,Security
Apache JIRA(s): None
Attachment: None
Last Updated: 23/6/2023, 4:18:10 pm
First Published: 23/6/2023, 2:20:44 pm
Summary
When configuring NiFi Authorizers.xml UserGroupProvider section, It is a common question to decide which Composite provider is required to be configured between CompositeUserGroupProvider vs CompositeConfigurableUserGroupProvider.
Instructions
The main difference between the two is:

CompositeUserGroupProvider: Does Not allow users to modify/delete/update users or groups details in NiFi UI -->HamBurger Menu-->Users Page which gets stored in the users.xml file CompositeConfigurableUserGroupProvider: Allows users to modify/delete/update users or groups details in NiFi UI -->HamBurger Menu-->Users Page which gets stored in the users.xml file

Which One to choose and when :

When using File based access policy provider, it is required that new users/groups must be added to NiFi users.xml file if users are not synced from LDAP for the policy or if a different NiFi cluster node identity needs to be added in NiFi users.xml file for which Authorization policy needs to be granted when using Site to Site in this case CompositeConfigurableUserGroupProvider is required.
If NiFi Authorization policy is governed through Ranger then required users can be added in Ranger as local if not synced from LDAP or for Site to Site, other NiFi cluster nodes can be added in Ranger as Node identity local users, and policy can be granted, No action required in NiFi UI since userx.xml file is Not in use thus CompositeUserGroupProvider is required to be configured.

See Also
Ref: https://docs.cloudera.com/cfm/2.1.5/cfm-security/topics/cfm-security-ldap-file-based-authorizer.html
Ref: https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#composite-implementations



\n 



Impala ODBC connector connection failure
Labels: Administration ,CDP Data Center ,CDP Private Cloud Base ,Configure ,Impala
Apache JIRA(s): None
Attachment: None
Last Updated: 21/6/2023, 9:24:42 pm
First Published: 1/4/2023, 3:32:21 pm
Summary
This article describes why below ODBC connection issue occurs and how tackle it. ""Simba::ImpalaODBC::ImpalaTCLIServiceThreadSafeClient::~ImpalaTCLIServiceThreadSafeClient: Error occurred while closing session or cleaning up TCLIService client: No more data to read.""
Symptoms
Impala ODBC driver connection fails with below error:
---
Simba::ImpalaODBC::ImpalaTCLIServiceThreadSafeClient::~ImpalaTCLIServiceThreadSafeClient: Error occurred while closing session or cleaning up TCLIService client: No more data to read.
Sep 01 19:34:31.169 ERROR 16024 Simba::ODBC::StatementStateAllocated::SQLExecDirectW: [Cloudera][ImpalaODBC] (450) Error when calling the Impala Thrift API ExecuteStatement: No more data to read.
Sep 01 19:34:31.169 TRACE 16024 Simba::ImpalaODBC::ImpalaDataEngine::~ImpalaDataEngine: +++++ enter +++++
Sep 01 19:34:31.169 ERROR 16024 Simba::ODBC::Statement::SQLExecDirectW: [Cloudera][ImpalaODBC] (450) Error when calling the Impala Thrift API ExecuteStatement: No more data to read.
Sep 01 19:34:31.171 TRACE 16024 Simba::ODBC::Connection::SQLGetConnectAttr: +++++ enter +++++
Sep 01 19:34:31.171 INFO 16024 Simba::ODBC::Connection::SQLGetConnectAttr: Attribute:

---

Cause
This issue occurred because the client/driver tried to call ExecuteStatement() on a connection that was already closed on the server-side after the 30s of inactivity.

Instructions
Follow the below steps and set the property:

CM -> Impala -> Configuration -> Impala Daemon command line safety valve:
-idle_client_poll_period_s=0



\n 



Errors connecting to HMS using thrift on kerberized environments
Labels: Cloudera Data Platform (CDP) ,Configure ,Development ,Hive ,MapReduce 
Apache JIRA(s): None
Attachment: None
Last Updated: 20/6/2023, 12:34:34 pm
First Published: 14/6/2023, 8:14:22 pm
Summary
Jobs connecting to HMS using HCatUtil.getHiveClient on a kerberized environment fail to connect due to missing credentials on newly added nodes. Job works on the rest of the nodes
Symptoms
On newly deployed nodes executing TDCH imports/exports, job fails with the following stacktrace

hadoop jar teradata-connector-1.8.4.1.jar com.teradata.hadoop.tool.TeradataImportTool ....
...
2023-05-31 15:43:07,559 INFO conf.HiveConf: Found configuration file file:/des_code/prd/DES_MASTER/SCRIPTS/HiveConf/hive-site.xml
2023-05-31 15:43:07,940 INFO metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2023-05-31 15:43:07,947 INFO metastore.HiveMetaStoreClient: Trying to connect to metastore with URI thrift://hivemetastore.example.com:9083
2023-05-31 15:43:07,958 INFO metastore.HiveMetaStoreClient: HMSC::open(): Could not find delegation token. Creating KERBEROS-based thrift connection.
2023-05-31 15:43:08,080 ERROR transport.TSaslTransport: SASL negotiation failure
javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
 at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:211)
 at org.apache.thrift.transport.TSaslClientTransport.handleSaslStartMessage(TSaslClientTransport.java:94)
 at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:271)
 at org.apache.thrift.transport.TSaslClientTransport.open(TSaslClientTransport.java:37)
 at org.apache.hadoop.hive.metastore.security.TUGIAssumingTransport$1.run(TUGIAssumingTransport.java:51)
 at org.apache.hadoop.hive.metastore.security.TUGIAssumingTransport$1.run(TUGIAssumingTransport.java:48)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:422)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1898)
 at org.apache.hadoop.hive.metastore.security.TUGIAssumingTransport.open(TUGIAssumingTransport.java:48)
 at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:637)
 at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.(HiveMetaStoreClient.java:244)
 at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.(HiveMetaStoreClient.java:145)
 at com.teradata.connector.hive.utils.HiveUtils.isHiveOutputTablePartitioned(HiveUtils.java:1395)
 at com.teradata.connector.common.tool.ConnectorImportTool.processArgs(ConnectorImportTool.java:782)
 at com.teradata.connector.common.tool.ConnectorImportTool.run(ConnectorImportTool.java:72)
 at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
 at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)
 at com.teradata.hadoop.tool.TeradataImportTool.main(TeradataImportTool.java:34)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.apache.hadoop.util.RunJar.run(RunJar.java:318)
 at org.apache.hadoop.util.RunJar.main(RunJar.java:232)
SQOOP jobs could also show a similar stacktrace

sqoop -import  ...  --hcatalog-database <DB> --hcatalog-table <TABLE> ...
2023-06-07 08:34:13,516 WARN common.HCatUtil: HCatUtil.getHiveClient is unsafe and can be a resource leak depending on HMSC implementation and caching mechanism. Use HCatUtil.getHiveMetastoreClient instead.
2023-06-07 08:34:13,598 INFO metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2023-06-07 08:34:13,608 INFO metastore.HiveMetaStoreClient: Trying to connect to metastore with URI thrift://hivemetastore.example.com:9083
2023-06-07 08:34:13,684 INFO metastore.HiveMetaStoreClient: HMSC::open(): Could not find delegation token. Creating KERBEROS-based thrift connection.
2023-06-07 08:34:13,821 ERROR transport.TSaslTransport: SASL negotiation failure
javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
 at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:211)
 at org.apache.thrift.transport.TSaslClientTransport.handleSaslStartMessage(TSaslClientTransport.java:94)
 at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:271)
 at org.apache.thrift.transport.TSaslClientTransport.open(TSaslClientTransport.java:37)
 at org.apache.hadoop.hive.metastore.security.TUGIAssumingTransport$1.run(TUGIAssumingTransport.java:51)
 at org.apache.hadoop.hive.metastore.security.TUGIAssumingTransport$1.run(TUGIAssumingTransport.java:48)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:422)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1898)
 at org.apache.hadoop.hive.metastore.security.TUGIAssumingTransport.open(TUGIAssumingTransport.java:48)
 at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:637)
 at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:244)
 at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:145)
 at org.apache.hive.hcatalog.common.HCatUtil.getHiveClient(HCatUtil.java:595)
 at org.apache.sqoop.mapreduce.hcat.SqoopHCatUtilities.isHCatView(SqoopHCatUtilities.java:181)
 at org.apache.sqoop.tool.BaseSqoopTool.validateHCatalogOptions(BaseSqoopTool.java:1624)
 at org.apache.sqoop.tool.ImportTool.validateOptions(ImportTool.java:1232)
 at org.apache.sqoop.Sqoop.run(Sqoop.java:141)
 at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
 at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:187)
 at org.apache.sqoop.Sqoop.runTool(Sqoop.java:241)
 at org.apache.sqoop.Sqoop.runTool(Sqoop.java:250)
 at org.apache.sqoop.Sqoop.main(Sqoop.java:259)
Caused by: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)
 at sun.security.jgss.krb5.Krb5InitCredential.getInstance(Krb5InitCredential.java:147)
 at sun.security.jgss.krb5.Krb5MechFactory.getCredentialElement(Krb5MechFactory.java:122)
 at sun.security.jgss.krb5.Krb5MechFactory.getMechanismContext(Krb5MechFactory.java:187)
 at sun.security.jgss.GSSManagerImpl.getMechanismContext(GSSManagerImpl.java:224)
 at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:212)
 at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:179)
 at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:192)
 ... 22 more
2023-06-07 08:34:13,826 WARN metastore.HiveMetaStoreClient: Failed to connect to the MetaStore Server...
2023-06-07 08:34:13,827 INFO metastore.HiveMetaStoreClient: Trying to connect to metastore with URI thrift://hivemetastore.example.com:9083
2023-06-07 08:34:13,827 INFO metastore.HiveMetaStoreClient: HMSC::open(): Could not find delegation token. Creating KERBEROS-based thrift connection.
2023-06-07 08:34:13,829 ERROR transport.TSaslTransport: SASL negotiation failure
Instructions
The problem is because the new nodes DOES NOT have HDFS / YARN / TEZ/ Hive on TEZ / Hive Metastore Gateway roles installed on the nodes, this causes the job to use the default role configurations.

Form Cloudera Manager, assign all these Gateway roles to the new nodes



\n 



Check TLS/SSL configuration of CDSW
Labels: CDP Data Center ,Cloudera Data Science Workbench ,Configure
Apache JIRA(s): None
Attachment: None
Last Updated: 16/6/2023, 11:15:08 am
First Published: 25/5/2023, 7:15:26 pm
Summary
Initial configuration and update of TLS certificates is error prone, and often results connection issues. Here are a couple of tests which can be done in case of TLS errors.
Symptoms
If certificates are wrong, users cannot connect to CDSW.

Cause
Certficates and root certificate need to be in place and match to each other.

Instructions
There are three files needed to configure TLS.

Content of all the three files should be provided by your security team.

The purpose of the three fields are:

Certificate: the certificate of the host, which the server sends to browsers (clients ) so that clients will be able to check it for validity
this is the public part of the public key encryption
it should point to a text file which starts like this
-----BEGIN CERTIFICATE-----
MIIEsDCCApigAwIBAgICEAEwDQYJKoZIhvcNAQELBQAwcDELMAkGA1UEBhMCSFUx
...

Key: the key which is used to decrypt/encrypt secret messages, it is the pair of the above certificate, it is the private part of the public key encryption, needs be kept in secret, available only for the server
it should point to a text file which starts like this
-----BEGIN PRIVATE KEY-----
MIIEvgIBADANBgkqhkiG9w0BAQEFAASCBKgwggSkAgEAAoIBAQDyFxtc2mLwKCmo
...

Root certificate authority: Lists of one or more certificates (usually two, but can be one, or more than two as well). It is able to verify the host certificate via the validation chain. It contains a self signed root certificate as well. It should be included in the browsers (usually done automatically with company security policy) and it is also used internally within cdsw when internal component connect to cdsw api.
it should point to a textfile which starts like this, but this file may contain more than one certificate
-----BEGIN CERTIFICATE-----
MIIEsDCCApigAwIBAgICEAEwDQYJKoZIhvcNAQELBQAwcDELMAkGA1UEBhMCSFUx
...

You can check these files with these commands whether they are correct:

Step 1) checking whether root certificate can validate the host certificate:
# openssl verify -CAfile rootcert.pem hostcert.pem
hostcert.pem: OK

Step 2) checking whether key and certificate belong together,
observe that the two outputs are the same

# openssl rsa -modulus -noout -in key.pem
Modulus=F2171B5CDA62F02829A...

# openssl x509 -modulus -noout -in cert.pem
Modulus=F2171B5CDA62F02829A...

Step 3) verify that the host certificate contains the cdsw domain and wildcard domain (*.<cdsw domain>)
# openssl x509 -noout -text -in cert.pem

observe the following content
...
Subject: ... CN=<hostname or cdsw domain>
...
X509v3 extensions:
X509v3 Subject Alternative Name:
DNS:*.<cdsw domain>, DNS:<cds



\n 



Is it ok to remove awscli, firefox, squid, thunderbird required for running Hadoop Operations on CDH 6?
Labels: Cloudera Enterprise Data Hub ,Configure ,HDFS ,Troubleshooting ,Tutorial
Apache JIRA(s): None
Attachment: None
Last Updated: 15/6/2023, 10:17:13 pm
First Published: 27/9/2022, 12:26:33 pm
Summary
The packages such as awscli, firefox, squid, thunderbird are not required for running any of the Components responsible for Hadoop ecosystem. You can consider removing these packages if not required. Packages mentioned below are the requirements with CDH 6, https://docs.cloudera.com/documentation/enterprise/6/6.3/topics/install_cloudera_packages.html#manual_install_cdh_packages
Instructions
Please make sure to look at the article below before performing the Package removals, 

https://docs.cloudera.com/documentation/enterprise/6/6.3/topics/install_cloudera_packages.html#manual_install_cdh_packages



\n 



In Workload XM, health checks like Metadata statistics do not show up when selecting a specific Impala job from the Workloads page
Labels: Administration ,Cloudera Workload XM (WXM) ,Configure ,Troubleshooting ,Workload XM
Apache JIRA(s): None
Attachment: None
Last Updated: 12/6/2023, 9:04:58 pm
First Published: 7/6/2023, 12:27:16 am
Summary
Metadata statistics are not appearing for a non-admin user.
Symptoms
Searching for queries using the Workload section WXM > Impala > Workloads > apply filters > select impala query will bring you to a unique Impala Query page built from Workloads with a URL like this, which is NOT showing the Metadata/Statistics section.

However, if you search for a query through other means like:

WXM Homepage > Impala > Usage Analysis > apply appropriate filters > select impala query
WXM Homepage > search icon > paste impala query id
Both of above steps will bring users to the normal Impala Query page with URL that shows the Metadata/Statistics section.

Cause
This is an expected behavior.

Health Checks in WXM are evaluated only if query execution time is greater than threshold duration (which is default to 5 seconds). In case of Workloads, the configured SLA is considered as this threshold duration (instead of default 5 seconds) i.e. some of the health checks will be executed only if the query execution time is greater than the configured SLA.

Instructions
View the Impala jobs via the below steps in WXM:

Go to WXM Homepage > Impala > Usage Analysis > apply appropriate filters > select impala query
OR
WXM Homepage > search icon > paste impala query ID
If you only have access to Workloads section, the above solution does not work:

Create a new Workload with the same criteria, but with a reduced SLA (like 5 seconds). This workaround allows users assigned to this workload to see the health checks in the metadata/statistics section.
You can create:
One workload intended to view the missed health checks, which may show many unintended missed SLA as a result of this change
Create another workload with the correct SLA to view all the queries that have missed SLA



\n 



ERROR: "Database schema is BAD! Check previous error log messages for details" failing to start Oozie Server
Labels: CDP Private Cloud ,Configure ,Development ,Installation ,Oozie
Apache JIRA(s): None
Attachment: None
Last Updated: 12/6/2023, 10:32:26 pm
First Published: 6/6/2023, 11:34:56 am
Summary
Oozie Server is failing to start after CDP upgrade. Error in the Oozie log: "Database schema is BAD! Check previous error log messages for details". This article will summarise how to resolve this particular error.
Symptoms
Unable to start Oozie Server after CDP 7.1.7 upgrade to higher version. Below error in the Oozie server log:

ERROR org.apache.oozie.command.SchemaCheckXCommand: SERVER[hostname] 
USER[-] GROUP[-] TOKEN[-] APP[-] JOB[-] ACTION[-] Found [12] missing tables: 
[SLA_REGISTRATION, OOZIE_SYS, BUNDLE_JOBS, OPENJPA_SEQUENCE_TABLE, VALIDATE_CONN, SLA_EVENTS, WF_JOBS, 
BUNDLE_ACTIONS, COORD_JOBS, SLA_SUMMARY, WF_ACTIONS, COORD_ACTIONS]
ERROR org.apache.oozie.command.SchemaCheckXCommand: SERVER[hostname] 
USER[-] GROUP[-] TOKEN[-] APP[-] JOB[-] ACTION[-] Found [12] extra tables: [bundle_jobs, sla_summary, 
sla_registration, openjpa_sequence_table, sla_events, wf_jobs, validate_conn, coord_actions, wf_actions, oozie_sys, coord_jobs, bundle_actions]
ERROR org.apache.oozie.command.SchemaCheckXCommand: SERVER[dev1.newchinalife.com] 
USER[-] GROUP[-] TOKEN[-] APP[-] JOB[-] ACTION[-] Database schema is BAD! Check previous error log messages for details
Applies to
After CDP 7.1.7 upgrade to Higher version.

Cause
The error Database table schema is misleading. This issue is caused by Oozie running out of memory. Oozie process folder shows OOM killed error. The Oozie heap size was not set at 50 MB after the upgrade.

Instructions
To resolve this issue, increase the Oozie heap size from 50 MB to 1G (or whatever suitable size according to the Cluster). Save the change and restart the Staled service.




\n 



Zeppelin takes too much to start/restart.
Labels: CDP Data Center ,CDP Private Cloud Base ,CDP Public Cloud ,Configure ,Zeppelin
Apache JIRA(s): None
Attachment: None
Last Updated: 7/6/2023, 10:52:44 pm
First Published: 26/5/2023, 6:00:29 pm
Summary
Zeppelin takes too much to start/restart.
Symptoms
We have installed in a new node Zeppelin. After configuration Zeppelin takes about 20 minutes to start finding an un-useful package:

/var/run/cloudera-scm-agent/process/1546340099-zeppelin-ZEPPELIN_SERVER/zeppelin-conf/helium.json does not exists

Connect to s3.amazonaws.com:443 [s3.amazonaws.com/XX.XX.XX.XX, s3.amazonaws.com/XX.XX.XX.XX, s3.amazonaws.com/XX.XX.XX.XX, s3.amazonaws.com/XX.XX.XX.XX, s3.amazonaws.com/XX.XX.XX.XX, s3.amazonaws.com/XX.XX.XX.XX, s3.amazonaws.com/XX.XX.XX.XX, s3.amazonaws.com/XX.XX.XX.XX] failed: Connection timed out (Connection timed out) 

Cause
This is because the Zeppelin by default try to reach AWS S3 for helium.json which is not even exist and it takes time for this search until it gets timeout.

Instructions
There are below two ways through which we can solve this issue.

1. To resolve this issue please do below steps to provide dummy helium value[With this option we will not be able to use Helium repositories.]:

CM > Zeppelin > Configuration >
Zeppelin Server Advanced Configuration Snippet (Safety Valve) for zeppelin-conf/zeppelin-site.xml
and add below entry:

Name : zeppelin.helium.registry
Value : helium

2. Add below configs to use the helium repositories[This changes only when we have outbound internet access]:

CM > Zeppelin > Configuration >
Zeppelin Server Advanced Configuration Snippet (Safety Valve) for zeppelin-conf/zeppelin-site.xml
and add below entry: 

zeppelin.helium.node.installer.url=https://nodejs.org/dist/
zeppelin.helium.npm.installer.url=http://registry.npmjs.org/
zeppelin.helium.registry=helium,https://s3.amazonaws.com/helium-package/helium.json
zeppelin.helium.yarnpkg.installer.url=https://github.com/yarnpkg/yarn/releases/download/



\n 



Unable to access hive databases from livy.sql in Zeppelin
Labels: CDP Private Cloud ,CDP Public Cloud ,Cloudera Data Platform (CDP) ,Configure ,Livy 
Apache JIRA(s): None
Attachment: None
Last Updated: 7/6/2023, 10:55:58 pm
First Published: 21/4/2023, 8:47:38 pm
Summary
Unable to access hive databases from livy.sql in Zeppelin it is only showing default database.
Symptoms
Unable to access hive databases from livy.sql in Zeppelin it is only showing default database.

Cause
Hive dependency was not added for Livy service.

Instructions
To resolve the issue please follow the steps below:

1. Open Cloudera Manager UI

2. Select Livy service

3. Click on configuration

4. Search for Hive

You will see HMS_SERVICE check box in livy configurations,  you have to check that[enable the hms_service]

5. Save configs and Restart Service.



\n 



Unable to Delete Collection in Solr due to Stream Body Disabled Error
Labels: CDP Private Cloud Base ,Configure ,Solr ,Troubleshooting
Apache JIRA(s): None
Attachment: None
Last Updated: 7/6/2023, 9:16:50 pm
First Published: 23/5/2023, 9:51:11 am
Summary
This article contains the steps to enable Remote Streaming and Stream Body
Symptoms
When attempting to delete a collection in Solr using the provided command,
curl --negotiate -u : -b ~/cookiejar.txt -c ~/cookiejar.txt ${SOLRHOST}/solr/${SOLRCOLLECTIONNAME}/update?stream.body=%3Cdelete%3E%3Cquery%3Efile_path:*${XYZ}%3C/query%3E%3C/delete%3E
the following error is encountered:
File path to be deleted from Solr index: hdfs://nameservice1/data/archive/<XYZ>{ "error":{ "metadata":[ "error-class","org.apache.solr.common.SolrException", "root-error-class","org.apache.solr.common.SolrException"], "msg":"Stream Body is disabled. See http://lucene.apache.org/solr/guide/requestdispatcher-in-solrconfig.html for help", "code":400}}{ "error": Unknown macro: { "metadata"}}
Instructions
To resolve the issue and enable the deletion of collections, the following steps should be followed:

Ensure that the enableRemoteStreaming and enableStreamBody properties are set to true in the Solr configuration. These properties enable remote streaming and body streaming, respectively.
Modify the Solr configuration using the provided method:
a. Execute the following command to retrieve the Solr instance directory:

# solrctl --jaas $(find /var/run/cloudera-scm-agent/process/SOLR -name jaas.conf|head -n1) instancedir --get <coll_name> /tmp/<coll_name>
b. Open the /tmp/<coll_name>/conf/solrconfig.xml file for editing.

c. Locate the <requestParsers> section within the file.

d. Set the enableRemoteStreaming and enableStreamBody attributes to "true":

<requestParsers enableRemoteStreaming="true" enableStreamBody="true" ... />
e. Save the changes to the solrconfig.xml file.

f. Update the Solr instance directory with the modified configuration:

# solrctl --jaas $(find /var/run/cloudera-scm-agent/process/SOLR -name jaas.conf|head -n1) instancedir --update <coll_name> /tmp/<coll_name>
g. Reload the collection for the changes to take effect:

# solrctl --jaas $(find /var/run/cloudera-scm-agent/process/SOLR -name jaas.conf|head -n1) collection --reload <coll_name>
By following these steps, the stream.body error should be resolved, and you should be able to delete the collection using the specified command.
Or, alternatively, run the below command to enable RemoteStreaming and BodyStreaming through the Config API:
# curl -H 'Content-type:application/json' -d '{"set-property": {"requestDispatcher.requestParsers.enableRemoteStreaming": true}, "set-property": {"requestDispatcher.requestParsers.enableStreamBody": true}}' http://<hostname>:<port>/solr/<collection_name>/config



\n 



How to set up local directories for Hadoop jobs accessing S3 using disk-based buffering
Labels: CDP Private Cloud ,Cloud ,Configure ,HDFS ,Security 
Apache JIRA(s): None
Attachment: None
Last Updated: 9/6/2023, 2:00:44 pm
First Published: 2/6/2023, 1:55:08 pm
Summary
While using an S3-based object store, Hadoop by default uses a local disk to buffer data to be uploaded to S3. The path used to store this buffer data is by default under the /tmp directory. This behavior may cause failures when using S3 and architectural issues in setting up a cluster to use S3 and security concerns. This article discusses the relevant path configurations used in disk-based buffering and provides a few considerations when setting up this mechanism in a production environment.
Symptoms
As described earlier, disk-based buffering is used when using S3 in Hadoop via the following configuration:

fs.s3a.fast.upload.buffer=disk
To manage the location (stored locally), use the fs.s3a.buffer.dir configuration which by default is set to ${hadoop.tmp.dir}/s3a. The value depends on the hadoop.tmp.dir configuration which by default is set to /tmp/hadoop-${user.name}. To summarize, the default path used to store buffered data would be /tmp/hadoop-${user.name}/s3a , where user.name would be the user running the job or process that accesses S3.

Note that fs.s3a.buffer.dir accepts multiple paths in a comma-separated manner.

The job will try and create this path and the buffer files as the user running the job which would mean that the user needs to have permission to create the appropriate directory and to read/write the appropriate files under /tmp. This requirement can pose a few problems:

We would need to either know which users would try and create these paths and create the relevant paths with appropriate permissions beforehand,
or give more extensive permissions for users on /tmp (or any other path we configure fs.s3a.buffer.dir). Giving complete access (777) to /tmp would resolve the issue, but due to security considerations, this may be excessive, while considering group-based permissions a more complex setup.
This issue becomes especially complex when using yarn applications (such as via distcp) as the local path would be used on any hosts running NodeManager instances.
/tmp may also not have sufficient space to support storing buffer data.
In summary, the default configurations used for disk-based buffering for S3 may be advised against when running Hadoop in a production environment.

Applies to
Any production environment in a Private Cloud accessing S3 (AWS or any other S3-compatible object store) where there is a stricter security policy, blocking from giving 777 permissions to directories.
Instructions
It is recommended to consider the following solutions when configuring fs.s3a.buffer.dir globally:

As noted above, give all users complete access (777 permission bits) to the path.
Limit the access to a specific unix group (770 permission bits for example) for the path to limit which users can use buffered data on disk.
Consider using alternative buffering mechanisms to store the buffer in memory. See Tuning S3A Uploads. 
Configure to ${env.LOCAL_DIRS:-${hadoop.tmp.dir}/s3a} or something similar in order to use a yarn applications localized directory for buffer data.
Use the user's home directory instead, configuring it to ${user.home}/s3a.
To elaborate on option 4., this should address most issues in using the buffer directory for any yarn application (or any custom process that sets the LOCAL_DIRS environment variable). For any yarn container that uses fs.s3a.buffer.dir , the LOCAL_DIRS environment variable usually would be set to the local application cache path[1] that should already have the appropriate user permissions. The :-${hadoop.tmp.dir}/s3a part defines the value fs.s3a.buffer.dir is set to if the LOCAL_DIRS environment variable is not set, using the default /tmp/hadoop-${user.name}/s3a path.

Note that one can use ${env.LOCAL_DIRS:-${hadoop.tmp.dir}}/s3a instead so that the buffer data would be stored under an s3a subdirectory in the application path, but this can cause inconsistencies with multiple directories set in the yarn.nodemanager.local-dirs configuration in Yarn as the variable substitution would only append the /s3a subdirectory to the last element in the list defined via the LOCAL_DIRS environment variable.

[1]

The path by default would be defined as below:
${yarn.nodemanager.local-dirs}/usercache/${user}/appcache/application_${appid}

See Also
For additional documentation and information, please see:

Cloudera documentation on S3 buffering:
Hadoop documentation on how Hadoop accesses S3, including details on the buffering mechanisms



\n 



How to setup JDBC Hive interpreter
Labels: CDP Data Center ,CDP Private Cloud ,CDP Private Cloud Base ,CDP Public Cloud ,Cloudera Data Platform (CDP) 
Apache JIRA(s): None
Attachment: None
Last Updated: 6/6/2023, 4:02:44 am
First Published: 4/5/2023, 12:10:59 pm
Summary
How to setup JDBC Hive interpreter
Symptoms
JDBC interpreter is missing from Zeppelin after installation. 

Instructions
To Create a JDBC[Hive] interpreter follow the steps mentioned below:

1. Check if zeppelin proxy user configs is present or not from the HDFS configs if it's not there please add the same as below:

CM-> HDFS -> Configurations -> Cluster-wide Advanced Configuration Snippet (Safety Valve) for core-site.xml

hadoop.proxyuser.zeppelin.hosts=*
hadoop.proxyuser.zeppelin.groups=*

Save and restarted all the required services.

Note: As we are making change in core-site.xml it will ask to restart all the services.

2. Then Login into Zeppelin and then create a new interpreter using below steps:

Open Zeppelin UI -> Login -> From top right corner click on Username -> Click on Interpreter Page -> Create [Click on create button]

Provide interpreter name [example Hive] and select interpreter group jdbc:

Interpreter Name: Hive
Interpreter Group: jdbc 

add/change below configs:
default.driver=org.apache.hive.jdbc.HiveDriver
default.password=<empty>
default.user=hive
default.url=jdbc:hive2://c467-node2.coelab.cloudera.com:2181,c467-node3.coelab.cloudera.com:2181,c467-node4.coelab.cloudera.com:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2
default.proxy.user.property=hive.server2.proxy.user
zeppelin.jdbc.auth.type=KERBEROS
zeppelin.jdbc.keytab.location= /<zeppelin-keytab-path>/zeppelin.keytab
zeppelin.jdbc.keytab.principal= <zeppelin-keytab-principal>
 
Save configs.

To use the newly created interpreter in existing notebooks we will need to bind the interpreter to the existing notebooks to do so we have to follow below steps:
Open any existing notebook -> From notebooks top right corner click on gear icon[setting icon] -> Then Select newly created interpreter Hive [It will turn blue from grey] -> Click on Save.
Once it's done we are good to use the Hive interpreter.



\n 



How to configure HDFS and Hive to use different JCEKS and different S3 buckets
Labels: Administration ,Cloud ,Cloudera Data Platform (CDP) ,Configure ,HDFS 
Apache JIRA(s): None
Attachment: None
Last Updated: 2/6/2023, 7:15:27 pm
First Published: 2/10/2021, 11:30:59 am
Summary
This article describes how to configure a cluster to use HDFS and Hive with S3 using JCEKS and different user credentials per S3 bucket.
Instructions
Do not handle AWS credentials in plaintext; the option to protect the AWS credentials is to use the JCEKS file. Here, we will use one JCEKS for each S3 bucket and user.

Perform the following steps:

Protect the AWS Credentials for the users and their S3 buckets:
hadoop credential create fs.s3a.access.key -value 12345 -provider jceks://hdfs/credpath/bucket-1.jceks

hadoop credential create fs.s3a.secret.key -value 67890 -provider jceks://hdfs/credpath/bucket-1.jceks
To verify:
hadoop credential list -provider jceks://hdfs/credpath/bucket-1.jceks
Use the same for another user JCEKS:
hadoop credential create fs.s3a.access.key -value 12345 -provider jceks://hdfs/credpath/bucket-3.jceks

hadoop credential create fs.s3a.secret.key -value 67890 -provider jceks://hdfs/credpath/bucket-3.jceks
To verify:
hadoop credential list -provider jceks://hdfs/credpath/bucket-3.jceks
If we want to configure HDFS and Hive:
hdfs dfs -mkdir /hivecreds
hdfs dfs -cp /credpath/* /hivecreds/
hdfs dfs -chown -R hive /hivecreds
Or, you can create a group, for example, group 'aws' and add HDFS and Hive users to that group, and then change the ownership:
hdfs dfs -chown -R hdfs:aws /credpath/
Test the credentials as HDFS user and Hive user:
hdfs dfs -Dfs.s3a.bucket.scc-803070-bucket-1.security.credential.provider.path=jceks://hdfs/credpath/bucket-1.jceks -ls s3a://scc-803070-bucket-1/

hdfs dfs -Dfs.s3a.bucket.scc-803070-bucket-3.security.credential.provider.path=jceks://hdfs/credpath/bucket-3.jceks -ls s3a://scc-803070-bucket-3/
If you get an error similar to:
ls: Cannot find password option fs.s3a.bucket.scc-803070-bucket-1.fs.s3a.server-side-encryption-algorithm
It means that the permission on the JCEKS file is not good.
Add to HDFS > core-site.xml
<property>
<name>fs.s3a.bucket.scc-803070-bucket-3.security.credential.provider.path</name>
<value>jceks://hdfs/credpath/bucket-3.jceks</value>
</property>
<property>
<name>fs.s3a.bucket.scc-803070-bucket-1.security.credential.provider.path</name>
<value>jceks://hdfs/credpath/bucket-1.jceks</value>
</property>
Uncheck the property "Generate HADOOP_CREDSTORE_PASSWORD" from Hive  service and Hive on Tez service. This is the flag to enable or disable the generation of HADOOP_CREDSTORE_PASSWORD (generate_jceks_password).
Restart required services.
Test HDFS access to the S3 bucket:
hdfs dfs -ls s3a://scc-803070-bucket-1/
hdfs dfs -ls s3a://scc-803070-bucket-3/
Connect to beeline:
beeline -u "jdbc:hive2://<HS2_FQDN>:10000/default"
Create an external table in S3:
CREATE EXTERNAL TABLE
external_s3_table_b1
(name STRING, type STRING) ROW FORMAT
DELIMITED FIELDS TERMINATED BY ','
LOCATION 's3a://scc-803070-bucket-1/test1/';
Insert data:
insert into external_s3_table_b1 values("Test1","S3_Bucket-1");